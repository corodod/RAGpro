# rag/generator.py
from __future__ import annotations
from dataclasses import dataclass
from typing import List, Dict, Optional

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
)


# ================= CONFIG =================
@dataclass
class GeneratorConfig:
    backend: str  # "cuda" | "cpu"
    max_new_tokens: int = 120
    temperature: float = 0.7
    repetition_penalty: float = 1.12


# ================= GENERATOR =================
class AnswerGenerator:
    def __init__(self, cfg: Optional[GeneratorConfig] = None):
        self.cfg = cfg or GeneratorConfig(backend="cpu")

        # -------- CUDA ‚Üí QWEN --------
        if self.cfg.backend == "cuda" and torch.cuda.is_available():
            self.scenario = "qwen"
            self.device = torch.device("cuda")
            self.model_name = "Qwen/Qwen2.5-3B-Instruct"
            self.dtype = torch.float16

        # -------- CPU / MAC ‚Üí TINYLLAMA --------
        else:
            self.scenario = "tinyllama"
            self.device = torch.device("cpu")
            self.model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
            self.dtype = torch.float32

            # üî¥ –ö–†–ò–¢–ò–ß–ù–û –î–õ–Ø MAC (segfault fix)
            torch.set_num_threads(1)

        print(
            f"[Generator] scenario={self.scenario} "
            f"model={self.model_name} device={self.device}"
        )

        # -------- LOAD TOKENIZER --------
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # -------- LOAD MODEL --------
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=self.dtype,
        )

        # -------- PIPELINE –¢–û–õ–¨–ö–û –î–õ–Ø TINYLLAMA --------
        if self.scenario == "tinyllama":
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=-1,  # CPU only
            )
        else:
            self.model.to(self.device)
            self.model.eval()
            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

    # ================= CONTEXT =================
    @staticmethod
    def build_context(chunks: List[Dict], max_chars: int = 4000) -> str:
        parts, total = [], 0
        for i, c in enumerate(chunks, start=1):
            text = (c.get("text") or "").strip()
            block = f"[{i}] {text}"
            if total + len(block) > max_chars:
                break
            parts.append(block)
            total += len(block) + 2
        return "\n\n".join(parts)

    # ================= PUBLIC API =================
    def generate(self, question: str, chunks: List[Dict]) -> str:
        if self.scenario == "tinyllama":
            return self._generate_tinyllama(question, chunks)
        else:
            return self._generate_qwen(question, chunks)

    # ================= TINYLLAMA =================
    def _generate_tinyllama(self, question: str, chunks: List[Dict]) -> str:
        context = self.build_context(chunks)

        prompt = (
            "Answer strictly using the facts from the context below.\n"
            "If the answer is not present, say:\n"
            "Not enough information in the context.\n\n"
            f"Context:\n{context}\n\n"
            f"Question:\n{question}\n\n"
            "Answer:"
        ).strip()

        out = self.pipe(
            prompt,
            max_new_tokens=self.cfg.max_new_tokens,
            do_sample=True,
            temperature=self.cfg.temperature,
            return_full_text=False,
            pad_token_id=self.tokenizer.eos_token_id,
        )[0]["generated_text"]

        return out.strip()

    # ================= QWEN =================
    def _generate_qwen(self, question: str, chunks: List[Dict]) -> str:
        context = self.build_context(chunks)
        # "–ù–ï –¥–æ–±–∞–≤–ª—è–π –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.\n"
        system = (
            "–¢—ã ‚Äî —Å–∏—Å—Ç–µ–º–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤.\n"
            "–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç–∞–º–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n"
            "–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî –Ω–∞–ø–∏—à–∏ —Ä–æ–≤–Ω–æ:\n"
            "–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        )

        user = (
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n"
            f"–í–æ–ø—Ä–æ—Å:\n{question}\n\n"
            "–û—Ç–≤–µ—Ç (2‚Äì3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):"
        )

        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ]

        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096,
        ).to(self.device)

        with torch.inference_mode():
            output = self.model.generate(
                **inputs,
                max_new_tokens=self.cfg.max_new_tokens,
                do_sample=False,
                repetition_penalty=self.cfg.repetition_penalty,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        gen_ids = output[0][inputs["input_ids"].shape[-1]:]
        answer = self.tokenizer.decode(gen_ids, skip_special_tokens=True)

        return answer.strip().split("\n\n")[0]
