# Retrieval-Augmented Generation с Coverage-Aware и Multi-Hop Retrieval

<img width="1203" height="640" alt="image_2026-02-07_01-58-30" src="https://github.com/user-attachments/assets/c63a0e94-785a-4fbc-adb5-2420747cce56" />



Данный проект представляет собой расширенную реализацию Retrieval-Augmented Generation (RAG), в которой основной акцент сделан на **качестве retrieval-слоя, управляемости пайплайна и устойчивости получаемого контекста**.  
В отличие от базовых RAG-реализаций, проект фокусируется не на генерации как таковой, а на систематическом улучшении этапов поиска, отбора и агрегации документов.

Архитектура сочетает классические методы информационного поиска, dense-подходы на базе sentence-transformers и LLM-усиленные техники (rewriting, planning), формируя **многоступенчатый и модульный retrieval-пайплайн**, пригодный для research- и engineering-задач.

---

## 1. Обзор применённых техник

### 1.1 Hybrid Retrieval

В основе retrieval-слоя лежит гибридный подход:

- **Sparse retrieval (BM25)**  
  Используется для обеспечения высокого recall и устойчивости к редким терминам.

- **Dense retrieval (Sentence Transformers + FAISS)**  
  Используется для семантического сопоставления запросов и документов.

Результаты sparse и dense retrieval объединяются с помощью **Reciprocal Rank Fusion (RRF)**, что позволяет:
- снизить чувствительность к ошибкам одного из методов,
- стабилизировать качество при сложных и переформулированных запросах,
- сохранить diversity кандидатов.

---

### 1.2 Dense Retrieval

Dense retrieval реализован на базе модели:

- **`intfloat/multilingual-e5-large`**

Особенности использования:
- строгая схема префиксов `query:` / `passage:`,
- cosine similarity как основная мера близости,
- FAISS как backend для хранения и поиска.

Dense retrieval применяется:
- для первичного поиска кандидатов,
- для дополнительного reranking candidate pool,
- как основа для coverage-aware selection.

---

### 1.3 Query Rewriting

Для улучшения recall и устойчивости retrieval используется query rewriting.

- Модель rewriting:
  - Qwen/Qwen3-4B-Instruct-2507 (если доступен мощный GPU)
  - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (fallback для слабых устройств)

Механизм:
- LLM генерирует несколько альтернативных формулировок исходного запроса,
- каждая rewrite-версия проходит **cosine-фильтрацию** относительно исходного запроса (на эмбеддингах E5),
- сохраняются только семантически близкие варианты.

Цель:
- расширить пространство поиска,
- избежать semantic drift,
- не допустить доминирования LLM над исходным запросом.

---

### 1.4 Query2Doc и HyDE

На этапе понимания запроса система формирует дополнительные retrieval-сигналы.
Большинство компонентов включается флагами и дополнительно гейтится правилами качества.

#### Query2Doc
- LLM генерирует псевдо-документ, описывающий ожидаемый ответ,
- используется как дополнительный retrieval-сигнал,
- активируется только при деградации базового retrieval.

#### HyDE
- используется как fallback-механизм,
- строго ограничен и гейтится условиями качества,
- не применяется по умолчанию.

Модель:
- **та же instruct-модель, что и для rewriting**

---

### 1.5 Entity-Aware Retrieval

Для повышения точности retrieval при наличии явных сущностей реализован entity-aware retrieval, который извлекает из текста только релевантные, самодостаточные сущности.

- Используется комбинация подходов:
  - NER через spaCy — доверенные сущности из моделей типа ru_core_news_lg.
  - POS-фильтрованные именные группы — конструкции типа ADJ* + NOUN+, проходящие структурные проверки.
  - Многословные слова с заглавной буквы и аббревиатуры, которые часто не попадают в NER.
- Извлечённые сущности фильтруются по длине, числу токенов, типу (только PERSON, ORG, GPE, LOC, EVENT, WORK_OF_ART) и синтаксической релевантности (не объекты прямого дополнения, не глагольные конструкции).
- Entity-based retrieval активируется выборочно, сущности используются как дополнительный сигнал при формировании candidate pool и coverage-aware selection.

Подход позволяет:
- повысить precision для entity-centric вопросов,
- избежать шумного расширения запроса.

---

### 1.6 Cross-Encoder Reranking

Для точного переупорядочивания кандидатов используется cross-encoder.

- Модель:
  - **`cross-encoder/ms-marco-MiniLM-L-6-v2`** (или эквивалент, задаётся конфигурацией)

Особенности:
- применяется только к ограниченному candidate pool,
- используется порог по score,
- является опциональным этапом.

Цель:
- повысить precision,
- сохранить вычислительную эффективность,
- не снижать diversity retrieval-результатов.

---

### 1.7 Coverage-Aware Selection

Вместо наивного top-k по score реализован **coverage-aware greedy selector**, оптимизирующий семантическое покрытие вопроса.

Используемая метрика покрытия:

**`coverage = α · cos(q, mean(C)) + (1 − α) · maxi cos(q, ci)`**


Где:
- **q** — эмбеддинг запроса,
- **C** — множество выбранных чанков,
- **c**i — отдельный чанк.

Свойства алгоритма:
- детерминированный,
- не требует обучения,
- интерпретируемый,
- останавливается при малом маржинальном приросте покрытия.

---

### 1.8 Semantic Chunking и NER

Для обеспечения качественного retrieval и поддержки coverage-aware selection все документы предварительно обрабатываются через semantic chunking:

- Разделение на смысловые чанки. Длинные документы делятся на семантически согласованные блоки по предложениям с помощью sentence-transformers.
  - Минимальный размер чанка: 120 символов
  - Максимальный размер: 500 символов
  - Слияние предложений происходит до достижения порога семантической схожести (cosine ≥ 0.58)

- Извлечение сущностей (NER). Для длинных документов извлекаются именованные сущности (PERSON, ORG, LOC, EVENT) с помощью модели ekaterinatao/nerel-bio-rubert-base.
  - Сущности используются как дополнительный retrieval-сигнал.
  - Short-документы NER не проходят, но могут получать сущности соседних long-документов при включённой опции.

- Short-doc merge. Короткие документы (например, заголовки или аннотации) могут быть объединены с контекстом соседних чанков, чтобы улучшить полноту информации:
  - [КОНТЕКСТ_ПРЕД] – предыдущий чанк
  - [КОНТЕКСТ_СЛЕД] – следующий чанк
  - Ограничение итогового размера текста: 1200 символов
  - Сущности соседних чанков могут быть присоединены к short-докам, если включена соответствующая опция

- Эмбеддинги и фильтрация. Каждый чанк кодируется через Sentence Transformer (intfloat/multilingual-e5-small), что обеспечивает корректное вычисление cosine similarity для reranking и coverage-aware selection.

---

### 1.9 Agentic Retrieval (Decompose → Compile → Execute)

Для поддержки multi-hop и compositional вопросов в проекте реализован **agent executor**, который строит и исполняет управляемый retrieval-план.

В отличие от итеративных multi-hop циклов, основанных на эвристиках или стабилизации ответа, данный подход использует **двухстадийное LLM-планирование с последующим детерминированным исполнением**, что обеспечивает:

- воспроизводимость пайплайна,
- строгий контроль операций,
- прозрачность и отладочность каждого шага retrieval.

---

#### 1.9.1 Общая архитектура

Agentic retrieval состоит из трёх логических фаз:

1. **Decompose** — разложение вопроса на под-задачи (LLM).
2. **Compile** — компиляция под-задач в исполняемый план (LLM).
3. **Execute** — детерминированное выполнение плана (non-LLM).

LLM используется **только на этапах планирования**, все retrieval- и aggregation-операции выполняются строго по заранее определённому плану.

---

#### 1.9.2 Decomposer (LLM, Stage L1)

На первом этапе исходный вопрос преобразуется в небольшой ориентированный ациклический граф (DAG) под-вопросов `Q1..Qn`.

Каждый под-вопрос описывается:

- текстом под-запроса,
- списком зависимостей (`deps`),
- опциональной slot-переменной (`slot`), если результат этого под-вопроса должен быть использован далее.

Slot используется для моделирования зависимостей вида *«сначала определить X, затем спросить про X»*.

Примеры ситуаций, где используется slot:

- относительные конструкции (*«который…», «тот, что…»*),
- скрытые сущности (*«убийство, которое…», «клуб, который…»*),
- двухшаговые factoid-вопросы.

Количество под-вопросов и зависимостей строго ограничено для предотвращения переусложнения плана.

---

#### 1.9.3 Decomposition Validator (rule-based)

Полученный DAG проходит строгую rule-based валидацию:

- количество под-вопросов ограничено (1–5),
- число зависимостей на узел ограничено,
- запрещены циклы,
- каждый slot:
  - определяется ровно один раз,
  - обязательно используется хотя бы в одном другом под-вопросе,
  - может использоваться только при наличии зависимости от producer-узла.

При некорректной декомпозиции система либо инициирует повторную генерацию, либо деградирует к одношаговому retrieval без агентного режима.

---

#### 1.9.4 Compiler (LLM, Stage L2)

Валидный граф под-вопросов компилируется во внутреннее представление **CompiledPlan (JSON)**.

На этом этапе явно задаётся:

- какие под-вопросы выполняются,
- какие результаты являются источниками retrieval (`hits*`),
- стратегия агрегации (`union` / `intersect`),
- источник финального ответа (`synth_from`),
- максимальный объём доказательств (`max_evidence`).

Таким образом, логика multi-hop retrieval становится **явно закодированной**, а не имплицитной.

---

#### 1.9.5 JSON → DSL → Plan (non-LLM)

Скомпилированный JSON-план переводится в линейный DSL, состоящий из атомарных операций.

DSL затем парсится в исполняемый `Plan`, который:

- не содержит LLM-зависимостей,
- допускает строгую валидацию,
- может быть логирован, воспроизведён и отлажен.

Этот шаг служит границей между вероятностным и детерминированным участками системы.

---

#### 1.9.6 Executor (deterministic runtime)

Executor последовательно выполняет шаги плана, ведя явное состояние (`state`) и применяя защитные ограничения:

- clamp параметров (`top_k`, `fanout`, `max_entities`, `threshold`),
- проверка наличия зависимостей перед выполнением шага,
- кеширование retrieval-результатов.

Поддерживаемые операции плана:

- **RETRIEVE** — гибридный retrieval через base retriever,
- **EXTRACT_ANSWER** — извлечение короткого значения из контекста (строго JSON),
- **COMPOSE_QUERY** — формирование уточнённого запроса с подстановкой `{x}`,
- **UNION_HITS / INTERSECT_HITS** — агрегация retrieval-результатов,
- **SYNTHESIZE** — финальный ответ строго по собранному контексту.

Дополнительно реализованы entity-based операции (`EXTRACT_ENTITIES*`, `MAP_RETRIEVE`, `FILTER_CE`), которые используются **только при наличии соответствующих шагов в плане**.

---

#### 1.9.7 Типовой сценарий (slot-based multi-hop)

Наиболее распространённый сценарий агентного retrieval выглядит следующим образом:

1. Найти объект **X** (например, клуб, событие, организацию) → `RETRIEVE`
2. Извлечь **X** из контекста → `EXTRACT_ANSWER`
3. Сформировать уточнённый запрос с подстановкой **X** → `COMPOSE_QUERY`
4. Выполнить второй retrieval → `RETRIEVE`
5. Объединить результаты → `UNION_HITS`
6. Синтезировать финальный ответ по доказательствам → `SYNTHESIZE`

Такой подход позволяет обрабатывать вопросы с неявными зависимостями, сохраняя контроль над retrieval-процессом и избегая генеративного дрейфа.



---

## 2. Подробное описание пайплайна

### Шаг 1. Входной запрос

Пользовательский вопрос поступает в систему без предварительной модификации.

---

### Шаг 2. Query Understanding

На этапе понимания запроса выполняется:
- генерация rewrite-вариантов,
- извлечение сущностей,
- оценка необходимости Query2Doc или HyDE.

Все LLM-усиления строго гейтятся метриками сходства.

---

### Шаг 3. Hybrid Retrieval

Для исходного запроса и допустимых rewrite-версий выполняется:
- BM25 retrieval,
- dense retrieval (E5 + FAISS),
- объединение результатов через RRF.

---

### Шаг 4. Dense Reranking

Candidate pool:
- дополнительно переупорядочивается dense-моделью,
- эмбеддинги сохраняются для coverage-анализа.

---

### Шаг 5. Cross-Encoder Reranking (опционально)

При включении:
- cross-encoder применяется к ограниченному набору кандидатов,
- выполняется фильтрация по порогу качества.

---

### Шаг 6. Coverage-Aware Selection

Из кандидатов выбирается финальный набор чанков:
- с максимальным семантическим покрытием,
- минимальной избыточностью,
- ограниченным размером контекста.

---

### Шаг 7. Multi-Hop Loop (опционально)

Если включён агентный режим, retrieval выполняется не одним запросом, а через план, который строится LLM и исполняется детерминированным executor’ом.

Этапы:
- **Decompose:** LLM строит DAG под-вопросов (Q1..Qn), при необходимости задаёт slot-переменные. 
- **Validate:** rule-based проверка графа (лимиты, DAG, корректность slot usage). При ошибках — деградация до одного вопроса. 
- **Compile:** LLM компилирует DAG в CompiledPlan (JSON) с указанием merge-операции и источника synth_from. 
- **Translate:** JSON переводится в DSL-линии, затем парсится в Plan. 
- **Execute:** executor выполняет шаги (retrieve, extract_answer, compose_query, union/intersect, synthesize), применяет guardrails и (опционально) кеширует retrieval. 
- **Answer:** если агент уже выполнил SYNTHESIZE, результат используется напрямую; иначе генератор отвечает по top-k чанкам.

---

### Шаг 8. Answer Generation

Генерация ответа:
- использует только отобранные чанки,
- полностью отделена от retrieval-логики,
- не влияет на этапы поиска и планирования.

---

## 3. Используемые модели (сводка)

| Компонент                             | Модель                                                                 |
| ------------------------------------- | ---------------------------------------------------------------------- |
| Dense retrieval                       | `intfloat/multilingual-e5-large`                                       |
| Sparse retrieval                      | BM25                                                                   |
| Dense reranking                       | `intfloat/multilingual-e5-large`                                       |
| Cross-Encoder                         | `cross-encoder/ms-marco-MiniLM-L-6-v2`                                 |
| Query Rewriting                       | `Qwen/Qwen3-4B-Instruct-2507` или `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |
| Query2Doc / HyDE                      | `Qwen/Qwen3-4B-Instruct-2507` или `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |
| Agent Decomposer                      | `Qwen/Qwen3-4B-Instruct-2507` или `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |
| Agent Compiler                        | `Qwen/Qwen3-4B-Instruct-2507` или `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |
| Slot / Fact Extractor                 | `Qwen/Qwen3-4B-Instruct-2507` или `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |
| Answer Generation                     | `Qwen/Qwen3-4B-Instruct-2507` или `TinyLlama/TinyLlama-1.1B-Chat-v1.0` |
| Multi-Hop / Agent Executor            | rule-based executor + LLM planning                                     |
| Entity Extraction (corpus-time)       | `ekaterinatao/nerel-bio-rubert-base`                                   |
| Entity Extraction (runtime, optional) | spaCy `ru_core_news_lg`                                                |
| Semantic Chunking                     | `intfloat/multilingual-e5-small`                                       |

---

## 4. Конфигурация и управление

- Все ключевые параметры вынесены в `RetrieverConfig`,
- Каждый этап пайплайна может быть включён или отключён,
- Поддерживается debug-режим с полной трассировкой.

---

## 5. Запуск проекта

### Индексация

Предполагается предварительная подготовка:
- BM25 индекса,
- FAISS dense индекса.

### API

Проект разворачивается как FastAPI-сервис:

мощный ПК

```bash
$env:GEN_BACKEND="cuda"

python -m uvicorn api.app:app
```
MacBook M1
```bash
export GEN_BACKEND=cpu                                      
export TOKENIZERS_PARALLELISM=false
export OMP_NUM_THREADS=1
export MKL_NUM_THREADS=1

uvicorn api.app:app --host 127.0.0.1 --port 8000 --workers 1
```

