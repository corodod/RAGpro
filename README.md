# Retrieval-Augmented Generation с Coverage-Aware и Multi-Hop Retrieval

Данный проект представляет собой расширенную реализацию Retrieval-Augmented Generation (RAG), в которой основной акцент сделан на **качестве retrieval-слоя, управляемости пайплайна и устойчивости получаемого контекста**.  
В отличие от базовых RAG-реализаций, проект фокусируется не на генерации как таковой, а на систематическом улучшении этапов поиска, отбора и агрегации документов.

Архитектура сочетает классические методы информационного поиска, dense-подходы на базе sentence-transformers и LLM-усиленные техники (rewriting, planning), формируя **многоступенчатый, интерпретируемый retrieval-пайплайн**, пригодный для research- и engineering-задач.

---

## 1. Обзор применённых техник

### 1.1 Hybrid Retrieval

В основе retrieval-слоя лежит гибридный подход:

- **Sparse retrieval (BM25)**  
  Используется для обеспечения высокого recall и устойчивости к редким терминам.

- **Dense retrieval (Sentence Transformers + FAISS)**  
  Используется для семантического сопоставления запросов и документов.

Результаты sparse и dense retrieval объединяются с помощью **Reciprocal Rank Fusion (RRF)**, что позволяет:
- снизить чувствительность к ошибкам одного из методов,
- стабилизировать качество при сложных и переформулированных запросах,
- сохранить diversity кандидатов.

---

### 1.2 Dense Retrieval

Dense retrieval реализован на базе модели:

- **`intfloat/multilingual-e5-large`**

Особенности использования:
- строгая схема префиксов `query:` / `passage:`,
- L2-нормализация эмбеддингов,
- cosine similarity как основная мера близости,
- FAISS как backend для хранения и поиска.

Dense retrieval применяется:
- для первичного поиска кандидатов,
- для дополнительного reranking candidate pool,
- как основа для coverage-aware selection.

---

### 1.3 Query Rewriting

Для улучшения recall и устойчивости retrieval используется query rewriting.

- Модель rewriting:
  - **LLM-инструктор (Qwen / GPT-подобная instruct-модель, задаётся конфигурацией)**

Механизм:
- LLM генерирует несколько альтернативных формулировок исходного запроса,
- каждая rewrite-версия проходит **cosine-фильтрацию** относительно исходного запроса (на эмбеддингах E5),
- сохраняются только семантически близкие варианты.

Цель:
- расширить пространство поиска,
- избежать semantic drift,
- не допустить доминирования LLM над исходным запросом.

---

### 1.4 Query2Doc и HyDE

Дополнительно реализованы LLM-усиленные техники query expansion:

#### Query2Doc
- LLM генерирует псевдо-документ, описывающий ожидаемый ответ,
- используется как дополнительный retrieval-сигнал,
- активируется только при деградации базового retrieval.

#### HyDE
- используется как fallback-механизм,
- строго ограничен и гейтится условиями качества,
- не применяется по умолчанию.

Модель:
- **та же instruct-модель, что и для rewriting**

---

### 1.5 Entity-Aware Retrieval

Для повышения точности retrieval при наличии явных сущностей:

- извлекаются именованные сущности из запроса,
- сущности используются как дополнительный retrieval-сигнал,
- entity-based retrieval активируется выборочно.

Подход позволяет:
- повысить precision для entity-centric вопросов,
- избежать шумного расширения запроса.

---

### 1.6 Cross-Encoder Reranking

Для точного переупорядочивания кандидатов используется cross-encoder.

- Модель:
  - **`cross-encoder/ms-marco-MiniLM-L-6-v2`** (или эквивалент, задаётся конфигурацией)

Особенности:
- применяется только к ограниченному candidate pool,
- используется порог по score,
- является опциональным этапом.

Цель:
- повысить precision,
- сохранить вычислительную эффективность,
- не снижать diversity retrieval-результатов.

---

### 1.7 Coverage-Aware Selection

Вместо наивного top-k по score реализован **coverage-aware greedy selector**, оптимизирующий семантическое покрытие вопроса.

Используемая метрика покрытия:

\[
\text{coverage} = \alpha \cdot \cos(q, \text{mean}(C)) + (1 - \alpha) \cdot \max_i \cos(q, c_i)
\]

Где:
- \( q \) — эмбеддинг запроса,
- \( C \) — множество выбранных чанков,
- \( c_i \) — отдельный чанк.

Свойства алгоритма:
- детерминированный,
- не требует обучения,
- интерпретируемый,
- останавливается при малом маржинальном приросте покрытия.

---

### 1.8 Multi-Hop Retrieval

Для поддержки multi-hop вопросов реализован отдельный retrieval-цикл с планированием.

- Модель planner’а:
  - **LLM-инструктор (та же модель, что и для rewriting / HyDE)**

Особенности:
- planner оценивает достаточность текущего контекста,
- формирует уточняющие под-запросы,
- критерий остановки основан на **стабильности ответа**, а не на фиксированном числе шагов.

---

## 2. Подробное описание пайплайна

### Шаг 1. Входной запрос

Пользовательский вопрос поступает в систему без предварительной модификации.

---

### Шаг 2. Query Understanding

На этапе понимания запроса выполняется:
- генерация rewrite-вариантов,
- извлечение сущностей,
- оценка необходимости Query2Doc или HyDE.

Все LLM-усиления строго гейтятся метриками сходства.

---

### Шаг 3. Hybrid Retrieval

Для исходного запроса и допустимых rewrite-версий выполняется:
- BM25 retrieval,
- dense retrieval (E5 + FAISS),
- объединение результатов через RRF.

---

### Шаг 4. Dense Reranking

Candidate pool:
- дополнительно переупорядочивается dense-моделью,
- эмбеддинги сохраняются для coverage-анализа.

---

### Шаг 5. Cross-Encoder Reranking (опционально)

При включении:
- cross-encoder применяется к ограниченному набору кандидатов,
- выполняется фильтрация по порогу качества.

---

### Шаг 6. Coverage-Aware Selection

Из кандидатов выбирается финальный набор чанков:
- с максимальным семантическим покрытием,
- минимальной избыточностью,
- ограниченным размером контекста.

---

### Шаг 7. Multi-Hop Loop (опционально)

Если активирован multi-hop режим:
- planner анализирует достаточность контекста,
- при необходимости формируется дополнительный запрос,
- retrieval повторяется,
- процесс останавливается при стабилизации ответа.

---

### Шаг 8. Answer Generation

Генерация ответа:
- использует только отобранные чанки,
- полностью отделена от retrieval-логики,
- не влияет на этапы поиска и планирования.

---

## 3. Используемые модели (сводка)

| Компонент            | Модель |
|---------------------|--------|
| Dense retrieval     | `intfloat/multilingual-e5-large` |
| Cross-Encoder       | `cross-encoder/ms-marco-MiniLM-L-6-v2` |
| Query Rewriting     | Instruct LLM (Qwen / GPT-подобная) |
| Query2Doc / HyDE    | Instruct LLM |
| Multi-Hop Planner   | Instruct LLM |
| Answer Generation   | Instruct LLM |

---

## 4. Конфигурация и управление

- Все ключевые параметры вынесены в `RetrieverConfig`,
- Каждый этап пайплайна может быть включён или отключён,
- Поддерживается debug-режим с полной трассировкой.

---

## 5. Запуск проекта

### Индексация

Предполагается предварительная подготовка:
- BM25 индекса,
- FAISS dense индекса.

### API

Проект разворачивается как FastAPI-сервис:

```bash


мощный ПК
- $env:GEN_BACKEND="cuda"      
- python -m uvicorn api.app:app

MacBook M1
- export GEN_BACKEND=cpu                                      
- export TOKENIZERS_PARALLELISM=false
- export OMP_NUM_THREADS=1
- export MKL_NUM_THREADS=1

- uvicorn api.app:app --host 127.0.0.1 --port 8000 --workers 1

